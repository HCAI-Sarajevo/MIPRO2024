{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# %pip install keras-tuner --upgrade\n",
    "import keras_tuner\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from common_functions import *\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='val')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_dataset(data, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data)-look_back-1):\n",
    "        X.append(data[i:(i+look_back), 0])\n",
    "        Y.append(data[(i+look_back), 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "def build_model(hp, train_shape, look_back):\n",
    "    model = Sequential()\n",
    "    #  if hp is passed then tune hyperparameters\n",
    "    if hp:\n",
    "        model.add(LSTM(units=hp.Int('lstm_units_1', min_value=32, max_value=366, step=32), return_sequences=False, input_shape=(look_back, 1)))\n",
    "        model.add(Dense(1))\n",
    "        hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    else:\n",
    "        # return_sequences = True for stacked LSTM, where last LSTM layer must have return_sequences = False!\n",
    "        model.add(LSTM(units=100, return_sequences=False, input_shape=(look_back, 1)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def split_data(df, offset, train_size, test_size):\n",
    "    train_data = df.loc[df.index.min() + pd.DateOffset(days=offset) : df.index.min() + pd.DateOffset(days=offset) + pd.DateOffset(months=train_size)]\n",
    "\n",
    "    # If there is no number_of_test_values left in the df to the end, then train and test should be none\n",
    "    if train_data.index.max() + pd.DateOffset(days=test_size) > df.index.max():\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    test_data = df.loc[train_data.index.max() + pd.DateOffset(days=1) : train_data.index.max() + pd.DateOffset(days=test_size)]\n",
    "\n",
    "    start_date, slice_date, end_date = train_data.index.min(), train_data.index.max(), test_data.index.max()\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "\n",
    "    return train_data, test_data, scaler, start_date, slice_date, end_date\n",
    "\n",
    "def train_LSTM_model(train_data, look_back, batch_size, epochs, tune_hyperparameters):\n",
    "    # Prepare input and output for LSTM model\n",
    "    x_train, y_train = create_dataset(train_data, look_back)\n",
    "\n",
    "    # LSTM expects inputs to be shaped as (batch_size, timesteps, channels) or (num_samples, timesteps, features)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], look_back, 1))\n",
    "\n",
    "    # Splitting the last val_size of data for validation\n",
    "    val_size = int(0.1 * len(x_train))\n",
    "    x_train, y_train = x_train[:-val_size], y_train[:-val_size]\n",
    "    x_val, y_val = x_train[-val_size:], y_train[-val_size:]\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "    if tune_hyperparameters:\n",
    "        # Training the model\n",
    "        tuner = keras_tuner.Hyperband(\n",
    "                            # must be wrapped in lambda because build_model() takes more than hp as argument\n",
    "                            hypermodel=lambda hp: build_model(hp, x_train.shape[0], look_back),\n",
    "                            objective=[keras_tuner.Objective(\"val_loss\", \"min\")],\n",
    "                            max_epochs=10,\n",
    "                            overwrite=True,\n",
    "                            project_name='LSTM_hyperparameter_tuning'\n",
    "                            )\n",
    "        tuner.search(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[early_stop])\n",
    "        best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        print(\"\\nBest hyperparameters: \\n\")\n",
    "        print(best_hps.values)\n",
    "\n",
    "        model = tuner.hypermodel.build(best_hps)\n",
    "    else:\n",
    "        model = build_model(hp=None, train_shape=x_train.shape[0], look_back=look_back)\n",
    "        \n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, \n",
    "                        validation_data=(x_val, y_val), callbacks=[early_stop])\n",
    "    plot_loss(history)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def test_LSTM_model(model, train_data, test_data, look_back, scaler):\n",
    "    # Testing the model using sequential predictions\n",
    "    # making test set: x_test[0] = x_train[-5:]; x_test[1] = x_train[-4:],x_test[0]; x_test[2] = x_train[-3:],x_test[0],x_test[1] and so on\n",
    "    x_test = train_data[-look_back:]\n",
    "    y_test = test_data\n",
    "    testPredict = []\n",
    "\n",
    "    for i in range(0, len(test_data)):\n",
    "        x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "        x_test = np.reshape(x_test, (1, look_back, 1))\n",
    "        y_pred = model.predict(x_test)\n",
    "        # append y_pred value to testPredict\n",
    "        testPredict.append(y_pred[0][0])\n",
    "        # push the predictions to the end of the test set so it pushes out first value \n",
    "        x_test = np.append(x_test, y_pred[0][0])\n",
    "        # x_test = np.append(x_test, y_test[i][0])\n",
    "        x_test = x_test[1:]\n",
    "\n",
    "    testPredict = np.reshape(testPredict, (-1,1))\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, testPredict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, testPredict))\n",
    "    r2 = r2_score(y_test, testPredict)\n",
    "    smape = calculate_smape(y_test, testPredict)\n",
    "\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"R2 Score:\", r2)\n",
    "    print(\"SMAPE:\", smape)\n",
    "\n",
    "    # plot y_test and testPredict\n",
    "    plt.plot(y_test, label='Actual value', color='blue')\n",
    "    plt.plot(testPredict, label='Test Prediction', color='orange')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plot_boxplot(testPredict, y_test)\n",
    "\n",
    "    return rmse, mae, r2, smape, testPredict, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write experiment data to file\n",
    "dir_name = 'Results'\n",
    "file_name = 'LSTM_experiment_results.csv'\n",
    "results_file_path = os.path.join(dir_name, file_name)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "# Experiment parameters\n",
    "tune_hyperparameters = False\n",
    "epochs = 10\n",
    "\n",
    "# train_size represents the number of months to be used for training (e.g. 4*12 is 4 years, 6 is 0.5 years)\n",
    "# test_sizes represents the number of days to be used for testing (e.g. 4*7 is 4 weeks)\n",
    "experiment_cases = [\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_daily.csv\", 'label': \"AQI\", 'train_size': 4*12, 'test_sizes': [4*7]}, \n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_hourly.csv\", 'label': \"AQI\", 'train_size': 4*12, 'test_sizes': [4*7]},\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_daily_filled_with_neighbors.csv\", 'label': \"AQI\", 'train_size': 4*12, 'test_sizes': [4*7]},\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_hourly_filled_with_neighbors.csv\", 'label': \"AQI\", 'train_size': 4*12, 'test_sizes': [4*7]},\n",
    "\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_daily.csv\", 'label': \"AQI\", 'train_size': 1*12, 'test_sizes': [4*7]}, \n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_hourly.csv\", 'label': \"AQI\", 'train_size': 1*12, 'test_sizes': [4*7]},\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_daily_filled_with_neighbors.csv\", 'label': \"AQI\", 'train_size': 1*12, 'test_sizes': [4*7]},\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_hourly_filled_with_neighbors.csv\", 'label': \"AQI\", 'train_size': 1*12, 'test_sizes': [4*7]},\n",
    "\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_daily.csv\", 'label': \"AQI\", 'train_size': 0.5*12, 'test_sizes': [4*7]}, \n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_hourly.csv\", 'label': \"AQI\", 'train_size': 0.5*12, 'test_sizes': [4*7]},\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_daily_filled_with_neighbors.csv\", 'label': \"AQI\", 'train_size': 0.5*12, 'test_sizes': [4*7]},\n",
    "    {'input_dir': \"Dataset\", 'filename': \"dataset_aqi_hourly_filled_with_neighbors.csv\", 'label': \"AQI\", 'train_size': 0.5*12, 'test_sizes': [4*7]}\n",
    "]\n",
    "\n",
    "for experiment_case in experiment_cases:\n",
    "    input_dir = experiment_case['input_dir']\n",
    "    filename = experiment_case['filename']\n",
    "    label = experiment_case['label']\n",
    "    train_size = experiment_case['train_size']\n",
    "    test_sizes = experiment_case['test_sizes']\n",
    "\n",
    "    if 'hourly' in filename:\n",
    "        look_back = 5\n",
    "        batch_size = 128\n",
    "    if 'daily' in filename:\n",
    "        look_back = 3\n",
    "        batch_size = 30\n",
    "\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(os.path.join(input_dir,filename))\n",
    "    df.DateTime = pd.to_datetime(df.DateTime)\n",
    "    df = df.set_index('DateTime')\n",
    "\n",
    "    print(f'\\nRunning experiment for {filename} with label {label}:\\n')\n",
    "    for test_size in test_sizes:\n",
    "        offset = 0 # start from the beginning of the dataframe\n",
    "        while True:\n",
    "            train_data, test_data, scaler, start_date, slice_date, end_date = split_data(df, offset, train_size, test_size)\n",
    "\n",
    "            if train_data is None or test_data is None:\n",
    "                print(f'No more data for test size {test_size} days')\n",
    "                break\n",
    "\n",
    "            offset += 7 # move train+test window forward by 7 days for next iteration\n",
    "\n",
    "            model, history = train_LSTM_model(train_data, look_back, batch_size, epochs, tune_hyperparameters)\n",
    "\n",
    "            rmse, mae, r2, smape, testPredict, y_test = test_LSTM_model(model, train_data, test_data, look_back, scaler)\n",
    "\n",
    "            # Prepare data for CSV\n",
    "            data = {\n",
    "                'DateTime of experiment': [datetime.datetime.now()],\n",
    "                'Dataset used': [filename],\n",
    "                'Label predicted': [label],\n",
    "                'Training data from': [start_date],\n",
    "                'Training data to': [slice_date],\n",
    "                'Testing data from': [slice_date],\n",
    "                'Testing data to': [end_date],\n",
    "                'Test size': [len(test_data)],\n",
    "                'look_back': [look_back],\n",
    "                'batch_size': [batch_size],\n",
    "                'epochs': [epochs],\n",
    "                'Model configuration': [json.dumps(model.get_config())],\n",
    "                'Test Predictions': [testPredict.tolist()],\n",
    "                'Test Data': [y_test.tolist()],\n",
    "                'RMSE': [rmse],\n",
    "                'MAE': [mae],\n",
    "                'R2': [r2],\n",
    "                'SMAPE': [smape]\n",
    "            }\n",
    "\n",
    "            # print finished iteration\n",
    "            print(f'Finished iteration offset={offset} days train_size={train_size} months test_size={test_size} days\\n')\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df_result = pd.DataFrame(data)\n",
    "\n",
    "            # Write DataFrame to CSV\n",
    "            df_result.to_csv(results_file_path, mode='a', header=not os.path.exists(results_file_path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(results_file_path)\n",
    "\n",
    "# convert to datetime\n",
    "df_results['DateTime of experiment'] = pd.to_datetime(df_results['DateTime of experiment'])\n",
    "df_results['Training data from'] = pd.to_datetime(df_results['Training data from'])\n",
    "df_results['Training data to'] = pd.to_datetime(df_results['Training data to'])\n",
    "df_results['Testing data from'] = pd.to_datetime(df_results['Testing data from'])\n",
    "df_results['Testing data to'] = pd.to_datetime(df_results['Testing data to'])\n",
    "# convert Test Data and Test Predictions to lists of doubles\n",
    "df_results['Test Data'] = df_results['Test Data'].apply(lambda x: [item[0] for item in json.loads(x)])\n",
    "df_results['Test Predictions'] = df_results['Test Predictions'].apply(lambda x: [item[0] for item in json.loads(x)])\n",
    "# calculate time delta in months round to 0 decimals\n",
    "df_results['Training Duration'] = ((df_results['Training data to'] - df_results['Training data from']) / np.timedelta64(1, 'M')).round(0).astype(int)\n",
    "df_results['Testing duration'] = ((df_results['Testing data to'] - df_results['Testing data from']) / np.timedelta64(1, 'D')).round(0).astype(int)\n",
    "\n",
    "for experiment_case in experiment_cases:\n",
    "    filename = experiment_case['filename']\n",
    "    label = experiment_case['label']\n",
    "    train_size = experiment_case['train_size']\n",
    "    test_sizes = experiment_case['test_sizes']\n",
    "\n",
    "    print(f'\\nResults for {filename} with label {label}, train_size={train_size} months, test_size={test_sizes[0]} days:\\n')\n",
    "\n",
    "    df_results_filtered = df_results[(df_results['Dataset used'] == filename) & (df_results['Label predicted'] == label) & (df_results['Training Duration'] == train_size) & (df_results['Testing duration'] == test_sizes[0])]\n",
    "\n",
    "    # Columns that contain list of predictions for each experiment\n",
    "    tests_column = df_results_filtered['Test Data']\n",
    "    predictions_column = df_results_filtered['Test Predictions']\n",
    "\n",
    "    # Calculating errors for 1, 7, 14, 28 days and plotting forecast plot and boxplot\n",
    "    for i in [1, 7, 14, 28]:\n",
    "        # take i-1-th (-1 because enumerations starts from 0) element from each list of tests and predictions columns\n",
    "        tests = tests_column.apply(lambda x: x[i-1])\n",
    "        predictions = predictions_column.apply(lambda x: x[i-1])\n",
    "\n",
    "        # Calculate errors\n",
    "        mae = mean_absolute_error(tests, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(tests, predictions))\n",
    "        r2 = r2_score(tests, predictions)\n",
    "        smape = calculate_smape(tests, predictions)\n",
    "\n",
    "        print(f'Errors for {i} days:')\n",
    "        print(f'MAE: {mae}')\n",
    "        print(f'RMSE: {rmse}')\n",
    "        print(f'R2: {r2}')\n",
    "        print(f'SMAPE: {smape}')\n",
    "\n",
    "        # Plot forecast plot and boxplot\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(tests, label='Actual')\n",
    "        plt.plot(tests.index, predictions, label='Predicted')\n",
    "        plt.legend()\n",
    "        plt.ylabel(label)\n",
    "        plt.title('Actual vs Predicted ' + label)\n",
    "        plt.show()\n",
    "\n",
    "        plot_boxplot(predictions, tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
